{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "word embeddings represent words as low-dimensional vectors in mathematical space and capture their semantic and syntactic meaning.\n",
    "\n",
    "- Approaches:\n",
    "    - One-Hot Encoding\n",
    "    - Frequency-based\n",
    "    - Neural Networks\n",
    "\n",
    "the word embedding by means of NN can be made with Word2Vec, GloVe, Bert, GPT.\n",
    "\n",
    "- Word2Vec has 2 approaches:\n",
    "    - Continuous Bag of Words\n",
    "    - Skip-Gram\n",
    "\n",
    "NN based embeddings aim to:\n",
    "- capture context/meaning\n",
    "- capture similarity to other words\n",
    "- reduce dimension\n",
    "- avoid memory issues\n",
    "\n",
    "- Developed based on NN:\n",
    "\n",
    "corpus --> NN --> Word Embedding\n",
    "\n",
    "- Word2Vec:Continuous Bag of Words\n",
    "the key point is a sliding window where the center is the dependent word to be predicted\n",
    "\n",
    "we take 2 words before the dependent word, 2 after it, we take the embeddings, we make the average and then we give these values to a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe: Global Vectors for word representations\n",
    "\n",
    "based on co-occurrence matric of words in a corpus, which counts how often words appear together in the same context.\n",
    "\n",
    "so you construct a matrix of word co-occurrence counts \n",
    "and then factorize this matrix to obtain word embeddings.\n",
    "\n",
    "the factorization is made with SVD. the resulting embedding are dnese, low-dimensional vectors encoding words as vector of other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "is a pre-trained word embedding based on trasformers. applies masked language modelling masking some words in sentence and learn to predict them.\n",
    "\n",
    "applies next sentence prediction - is a model to predicts whether two sentences are similar in a text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT: Generative Pre-trained Transformers\n",
    "\n",
    "not strictly a word embedding but contextualized word embedding. Unique emedding for each occurrence of a word based on surrounding words in a text. Applies trasformers architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchtext.vocab as vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [03:20, 4.29MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:24<00:00, 16463.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# download pre-trained word vectors\n",
    "glove = vocab.GloVe(name = '6B', dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens 400000\n",
      "embedding space size 100\n"
     ]
    }
   ],
   "source": [
    "# number of words and embeddings\n",
    "glove.vectors.shape\n",
    "print(f'number of tokens {glove.vectors.shape[0]}\\nembedding space size {glove.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7635e-01,  5.4567e-01,  3.0534e-01,  9.0395e-01, -8.8172e-02,\n",
       "         6.2945e-01,  4.0376e-01, -8.1160e-01, -1.9370e-01, -3.1395e-01,\n",
       "        -1.6067e-02, -6.8291e-01, -1.2400e-02, -2.0827e-01, -1.0267e+00,\n",
       "         1.4386e+00,  5.1816e-01,  2.0026e-01, -8.3672e-04, -2.9563e-01,\n",
       "        -7.5463e-01,  1.9618e-01,  6.0900e-01,  3.6774e-01,  7.2106e-01,\n",
       "        -8.6832e-01, -2.1198e-01, -4.3051e-01,  7.1873e-01,  7.5019e-01,\n",
       "        -6.0245e-01,  7.5618e-01, -5.5033e-01, -6.6510e-01,  5.3047e-01,\n",
       "        -2.2391e-01, -9.2297e-01,  6.2659e-01, -2.5183e-01, -8.2082e-01,\n",
       "        -1.6507e-01,  2.9234e-01, -2.6373e-01, -8.1124e-01, -4.0006e-02,\n",
       "        -1.3341e-01,  2.9392e-01, -4.4894e-01,  5.6080e-02,  3.9754e-01,\n",
       "        -6.8598e-01, -3.4001e-01, -1.1112e-02,  7.5445e-01,  2.8091e-01,\n",
       "        -1.4169e+00,  2.7837e-01,  3.4846e-01,  1.3482e-01,  1.2508e+00,\n",
       "        -8.0446e-02,  4.9207e-01, -7.0844e-01,  6.3239e-01, -3.8550e-01,\n",
       "        -4.9367e-01, -2.1818e-01,  7.6461e-01,  6.3680e-01,  1.0494e-02,\n",
       "        -1.9562e-01, -2.6752e-01,  4.4512e-01,  5.6729e-01, -9.4051e-01,\n",
       "         1.0706e-01,  6.9933e-01, -3.4509e-02, -1.7194e-01,  5.0608e-02,\n",
       "         3.8976e-01, -3.2920e-01,  8.5001e-02, -3.3619e-01, -6.2415e-01,\n",
       "         3.5335e-01, -6.0966e-01, -3.6996e-01,  6.6216e-01, -1.5296e-01,\n",
       "         1.9905e-01,  9.6856e-01, -3.5779e-01,  5.3459e-01,  7.2205e-01,\n",
       "        -4.9331e-01, -3.8660e-02, -1.8411e-01,  2.2157e-01,  3.6662e-01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the embedding vector for a specific word\n",
    "def get_embedding_vector(word):\n",
    "    word_index = glove.stoi[word]\n",
    "    emb = glove.vectors[word_index]\n",
    "    return emb\n",
    "\n",
    "#example\n",
    "get_embedding_vector('chess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.0),\n",
       " ('dog', 2.681130886077881),\n",
       " ('rabbit', 3.648970603942871),\n",
       " ('cats', 3.6892004013061523),\n",
       " ('monkey', 3.7469322681427)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the closest words from input word\n",
    "def get_closest_words_from_word(word, max_n = 5):\n",
    "    word_emb = get_embedding_vector(word)\n",
    "    distances = [(w,torch.dist(word_emb , get_embedding_vector(w)).cpu().item()) for w in glove.itos]\n",
    "    dist_sorted_list = sorted(distances, key = lambda x : x[1])[:max_n]\n",
    "    return dist_sorted_list\n",
    "\n",
    "get_closest_words_from_word('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find closest words from embedding\n",
    "def get_closest_words_from_embedding(word_emb, max_n = 5):\n",
    "    distances = [(w, torch.dist(word_emb, get_embedding_vector(w)).cpu().item()) for w in glove.itos ]\n",
    "    dist_sorted_list = sorted(distances, key = lambda x: x[1])[:max_n]\n",
    "    return dist_sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 4.281251907348633),\n",
       " ('brother', 5.3268609046936035),\n",
       " ('thought', 5.396074295043945),\n",
       " ('son', 5.4332075119018555),\n",
       " ('father', 5.440552234649658)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find word analogies\n",
    "# for eaxmple king is to queen like man is to woman\n",
    "\n",
    "def get_word_analogy(word1, word2, word3, max_n = 5):\n",
    "    # w1 -w2 + w3 -> w4 analogy for w4\n",
    "    word_emb1 = get_embedding_vector(word1)\n",
    "    word_emb2 = get_embedding_vector(word2)\n",
    "    word_emb3 = get_embedding_vector(word3)\n",
    "\n",
    "    word_emb4 = word_emb1 - word_emb2 + word_emb3\n",
    "    analogy = get_closest_words_from_embedding(word_emb4, max_n = max_n)\n",
    "    return analogy\n",
    "get_word_analogy('king', 'queen','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
